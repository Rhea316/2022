{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "GraphFrame(v:[id: string, name: string ... 1 more field], e:[src: string, dst: string ... 1 more field])\n",
      "<class 'graphframes.graphframe.GraphFrame'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "\n",
    "sc = SparkContext(\"local\", appName=\"mysqltest\")\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "vertices = sqlContext.createDataFrame([\n",
    "  (\"a\", \"Alice\", 34),\n",
    "  (\"b\", \"Bob\", 36),\n",
    "  (\"c\", \"Charlie\", 30),\n",
    "  (\"d\", \"David\", 29),\n",
    "  (\"e\", \"Esther\", 32),\n",
    "  (\"f\", \"Fanny\", 36),\n",
    "  (\"g\", \"Gabby\", 60)], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "edges = sqlContext.createDataFrame([\n",
    "  (\"a\", \"b\", \"friend\"),\n",
    "  (\"b\", \"c\", \"follow\"),\n",
    "  (\"c\", \"b\", \"follow\"),\n",
    "  (\"f\", \"c\", \"follow\"),\n",
    "  (\"e\", \"f\", \"follow\"),\n",
    "  (\"e\", \"d\", \"friend\"),\n",
    "  (\"d\", \"a\", \"friend\"),\n",
    "  (\"a\", \"e\", \"friend\")\n",
    "], [\"src\", \"dst\", \"relationship\"])\n",
    "\n",
    "print(type(vertices))\n",
    "print(type(edges))\n",
    "\n",
    "# 生成图\n",
    "g = GraphFrame(vertices, edges)\n",
    "print(g)\n",
    "# GraphFrame(v:[id: string, name: string ... 1 more field], e:[src: string, dst: string ... 1 more field])\n",
    "print(type(g))\n",
    "# <class 'graphframes.graphframe.GraphFrame'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|inDegree|\n",
      "+---+--------+\n",
      "|  a|       1|\n",
      "|  b|       2|\n",
      "|  c|       2|\n",
      "|  d|       1|\n",
      "|  e|       1|\n",
      "|  f|       1|\n",
      "+---+--------+\n",
      "\n",
      "+---+---------+\n",
      "| id|outDegree|\n",
      "+---+---------+\n",
      "|  a|        2|\n",
      "|  b|        1|\n",
      "|  c|        1|\n",
      "|  d|        1|\n",
      "|  e|        2|\n",
      "|  f|        1|\n",
      "+---+---------+\n",
      "\n",
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  a|  Alice| 34|\n",
      "|  b|    Bob| 36|\n",
      "|  c|Charlie| 30|\n",
      "|  d|  David| 29|\n",
      "|  e| Esther| 32|\n",
      "|  f|  Fanny| 36|\n",
      "|  g|  Gabby| 60|\n",
      "+---+-------+---+\n",
      "\n",
      "+--------+\n",
      "|min(age)|\n",
      "+--------+\n",
      "|      29|\n",
      "+--------+\n",
      "\n",
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  d|David| 29|\n",
      "+---+-----+---+\n",
      "\n",
      "+---+---+------------+\n",
      "|src|dst|relationship|\n",
      "+---+---+------------+\n",
      "|  a|  b|      friend|\n",
      "|  b|  c|      follow|\n",
      "|  c|  b|      follow|\n",
      "|  f|  c|      follow|\n",
      "|  e|  f|      follow|\n",
      "|  e|  d|      friend|\n",
      "|  d|  a|      friend|\n",
      "|  a|  e|      friend|\n",
      "+---+---+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# g.inDegree()                #计算顶点的入度\n",
    "g.inDegrees.sort(\"id\").show()\n",
    "# g.outDegree()               #计算顶点的出度\n",
    "g.outDegrees.sort(\"id\").show()\n",
    "g.vertices.show()\n",
    "g.vertices.groupBy().min(\"age\").show()   #对顶点进行分组聚合\n",
    "g.vertices.filter(\"age < 30\").show()     #按条件筛选顶点\n",
    "g.edges.show()\n",
    "g.edges.filter(\"relationship = 'follow'\").count()  #按条件筛选边"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, name: string, age: bigint, pagerank: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#查找图中最重要的用户可以用pageRank函数：\n",
    "results = g.pageRank(resetProbability=0.15, maxIter=10)\n",
    "display(results.vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-------------------+\n",
      "| id|   name|age|           pagerank|\n",
      "+---+-------+---+-------------------+\n",
      "|  g|  Gabby| 60|0.17073170731707318|\n",
      "|  b|    Bob| 36| 2.7025217677349773|\n",
      "|  e| Esther| 32| 0.3613490987992571|\n",
      "|  a|  Alice| 34| 0.4485115093698443|\n",
      "|  f|  Fanny| 36|0.32504910549694244|\n",
      "|  d|  David| 29|0.32504910549694244|\n",
      "|  c|Charlie| 30| 2.6667877057849627|\n",
      "+---+-------+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.vertices.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------+------+\n",
      "|src|dst|relationship|weight|\n",
      "+---+---+------------+------+\n",
      "|  a|  b|      friend|   0.5|\n",
      "|  b|  c|      follow|   1.0|\n",
      "|  e|  f|      follow|   0.5|\n",
      "|  e|  d|      friend|   0.5|\n",
      "|  c|  b|      follow|   1.0|\n",
      "|  a|  e|      friend|   0.5|\n",
      "|  f|  c|      follow|   1.0|\n",
      "|  d|  a|      friend|   1.0|\n",
      "+---+---+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.edges.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------------+--------------+---------------+--------------+----------------+-----------+\n",
      "|              a|            ab|              b|            bc|              c|            cd|               d|num_friends|\n",
      "+---------------+--------------+---------------+--------------+---------------+--------------+----------------+-----------+\n",
      "| [d, David, 29]|[d, a, friend]| [a, Alice, 34]|[a, e, friend]|[e, Esther, 32]|[e, f, follow]|  [f, Fanny, 36]|          2|\n",
      "|[e, Esther, 32]|[e, d, friend]| [d, David, 29]|[d, a, friend]| [a, Alice, 34]|[a, e, friend]| [e, Esther, 32]|          3|\n",
      "| [d, David, 29]|[d, a, friend]| [a, Alice, 34]|[a, e, friend]|[e, Esther, 32]|[e, d, friend]|  [d, David, 29]|          3|\n",
      "| [d, David, 29]|[d, a, friend]| [a, Alice, 34]|[a, b, friend]|   [b, Bob, 36]|[b, c, follow]|[c, Charlie, 30]|          2|\n",
      "|[e, Esther, 32]|[e, d, friend]| [d, David, 29]|[d, a, friend]| [a, Alice, 34]|[a, b, friend]|    [b, Bob, 36]|          3|\n",
      "| [a, Alice, 34]|[a, e, friend]|[e, Esther, 32]|[e, d, friend]| [d, David, 29]|[d, a, friend]|  [a, Alice, 34]|          3|\n",
      "+---------------+--------------+---------------+--------------+---------------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Motif finding 用 Domain-Specific Language (DSL) 指定structural queries\n",
    "\n",
    "# Find chains of 4 vertices.\n",
    "chain4 = g.find(\"(a)-[ab]->(b); (b)-[bc]->(c); (c)-[cd]->(d)\")\n",
    " \n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import when\n",
    "# Query on sequence, with state (cnt)\n",
    "#  (a) Define method for updating state given the next element of the motif.\n",
    "def cumFriends(cnt, edge):\n",
    "  relationship = col(edge)[\"relationship\"]\n",
    "  return when(relationship == \"friend\", cnt + 1).otherwise(cnt)\n",
    " \n",
    "#  (b) Use sequence operation to apply method to sequence of elements in motif.\n",
    "#   In this case, the elements are the 3 edges.\n",
    "edges = [\"ab\", \"bc\", \"cd\"]\n",
    "numFriends = reduce(cumFriends, edges, lit(0))\n",
    "    \n",
    "chainWith2Friends2 = chain4.withColumn(\"num_friends\", numFriends).where(numFriends >= 2)\n",
    "chainWith2Friends2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+\n",
      "|               b|计数|\n",
      "+----------------+----+\n",
      "|  [a, Alice, 34]|   3|\n",
      "|    [b, Bob, 36]|   2|\n",
      "|[c, Charlie, 30]|   2|\n",
      "|  [d, David, 29]|   2|\n",
      "| [e, Esther, 32]|   2|\n",
      "|  [f, Fanny, 36]|   1|\n",
      "+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "chain4.groupBy(\"b\").agg(F.count(chain4.b).alias(\"计数\")).sort(\"b\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------------+--------------+----------------+--------------+----------------+\n",
      "|              a|            ab|              b|            bc|               c|            cd|               d|\n",
      "+---------------+--------------+---------------+--------------+----------------+--------------+----------------+\n",
      "| [d, David, 29]|[d, a, friend]| [a, Alice, 34]|[a, b, friend]|    [b, Bob, 36]|[b, c, follow]|[c, Charlie, 30]|\n",
      "| [d, David, 29]|[d, a, friend]| [a, Alice, 34]|[a, e, friend]| [e, Esther, 32]|[e, d, friend]|  [d, David, 29]|\n",
      "| [d, David, 29]|[d, a, friend]| [a, Alice, 34]|[a, e, friend]| [e, Esther, 32]|[e, f, follow]|  [f, Fanny, 36]|\n",
      "| [a, Alice, 34]|[a, b, friend]|   [b, Bob, 36]|[b, c, follow]|[c, Charlie, 30]|[c, b, follow]|    [b, Bob, 36]|\n",
      "|[e, Esther, 32]|[e, d, friend]| [d, David, 29]|[d, a, friend]|  [a, Alice, 34]|[a, b, friend]|    [b, Bob, 36]|\n",
      "|[e, Esther, 32]|[e, d, friend]| [d, David, 29]|[d, a, friend]|  [a, Alice, 34]|[a, e, friend]| [e, Esther, 32]|\n",
      "| [a, Alice, 34]|[a, e, friend]|[e, Esther, 32]|[e, f, follow]|  [f, Fanny, 36]|[f, c, follow]|[c, Charlie, 30]|\n",
      "| [a, Alice, 34]|[a, e, friend]|[e, Esther, 32]|[e, d, friend]|  [d, David, 29]|[d, a, friend]|  [a, Alice, 34]|\n",
      "+---------------+--------------+---------------+--------------+----------------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chain4.filter(\"ab.relationship = 'friend'\").sort(\"b\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------+\n",
      "|src|dst|relationship|\n",
      "+---+---+------------+\n",
      "|  d|  a|      friend|\n",
      "|  e|  d|      friend|\n",
      "|  d|  a|      friend|\n",
      "|  a|  e|      friend|\n",
      "|  f|  c|      follow|\n",
      "|  b|  c|      follow|\n",
      "|  d|  a|      friend|\n",
      "|  e|  f|      follow|\n",
      "|  c|  b|      follow|\n",
      "|  a|  b|      friend|\n",
      "|  e|  d|      friend|\n",
      "|  a|  e|      friend|\n",
      "+---+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chain4.select(\"ab.src\", \"ab.dst\", \"ab.relationship\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|inDegree|\n",
      "+---+--------+\n",
      "|  a|       3|\n",
      "|  b|       2|\n",
      "|  c|       2|\n",
      "|  d|       2|\n",
      "|  e|       2|\n",
      "|  f|       1|\n",
      "+---+--------+\n",
      "\n",
      "+---+---------+\n",
      "| id|outDegree|\n",
      "+---+---------+\n",
      "|  a|        3|\n",
      "|  b|        1|\n",
      "|  c|        1|\n",
      "|  d|        3|\n",
      "|  e|        3|\n",
      "|  f|        1|\n",
      "+---+---------+\n",
      "\n",
      "+---+---+------------+\n",
      "|src|dst|relationship|\n",
      "+---+---+------------+\n",
      "|  a|  b|      friend|\n",
      "|  b|  c|      follow|\n",
      "|  c|  b|      follow|\n",
      "|  f|  c|      follow|\n",
      "|  e|  f|      follow|\n",
      "|  e|  d|      friend|\n",
      "|  d|  a|      friend|\n",
      "|  a|  e|      friend|\n",
      "+---+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用顶点和边的集合构造子图\n",
    "e2 = chain4.select(\"ab.src\", \"ab.dst\", \"ab.relationship\")\n",
    "g2 = GraphFrame(g.vertices, e2)\n",
    "g2.inDegrees.sort(\"id\").show()\n",
    "g2.outDegrees.sort(\"id\").show()\n",
    "g.edges.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+----------------+\n",
      "|               A|             e|               B|\n",
      "+----------------+--------------+----------------+\n",
      "|  [d, David, 29]|[d, a, friend]|  [a, Alice, 34]|\n",
      "|  [d, David, 29]|[d, a, friend]|  [a, Alice, 34]|\n",
      "|  [d, David, 29]|[d, a, friend]|  [a, Alice, 34]|\n",
      "|[c, Charlie, 30]|[c, b, follow]|    [b, Bob, 36]|\n",
      "|  [a, Alice, 34]|[a, b, friend]|    [b, Bob, 36]|\n",
      "|  [f, Fanny, 36]|[f, c, follow]|[c, Charlie, 30]|\n",
      "|    [b, Bob, 36]|[b, c, follow]|[c, Charlie, 30]|\n",
      "| [e, Esther, 32]|[e, d, friend]|  [d, David, 29]|\n",
      "| [e, Esther, 32]|[e, d, friend]|  [d, David, 29]|\n",
      "|  [a, Alice, 34]|[a, e, friend]| [e, Esther, 32]|\n",
      "|  [a, Alice, 34]|[a, e, friend]| [e, Esther, 32]|\n",
      "| [e, Esther, 32]|[e, f, follow]|  [f, Fanny, 36]|\n",
      "+----------------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chain_i = g2.find(\"(A)-[e]->(B)\")\n",
    "chain_i.sort(\"B\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|inDegree|\n",
      "+---+--------+\n",
      "|  a|       3|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g2.inDegrees.filter(\"inDegree>2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+---+------------+----------------+\n",
      "| id|   name|src|dst|relationship|               b|\n",
      "+---+-------+---+---+------------+----------------+\n",
      "|  d|  David|  d|  a|      friend|  [a, Alice, 34]|\n",
      "|  e| Esther|  e|  d|      friend|  [d, David, 29]|\n",
      "|  d|  David|  d|  a|      friend|  [a, Alice, 34]|\n",
      "|  a|  Alice|  a|  e|      friend| [e, Esther, 32]|\n",
      "|  f|  Fanny|  f|  c|      follow|[c, Charlie, 30]|\n",
      "|  b|    Bob|  b|  c|      follow|[c, Charlie, 30]|\n",
      "|  d|  David|  d|  a|      friend|  [a, Alice, 34]|\n",
      "|  e| Esther|  e|  f|      follow|  [f, Fanny, 36]|\n",
      "|  c|Charlie|  c|  b|      follow|    [b, Bob, 36]|\n",
      "|  a|  Alice|  a|  b|      friend|    [b, Bob, 36]|\n",
      "|  e| Esther|  e|  d|      friend|  [d, David, 29]|\n",
      "|  a|  Alice|  a|  e|      friend| [e, Esther, 32]|\n",
      "+---+-------+---+---+------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chain4.select(\"a.id\",\"a.name\",\"ab.src\", \"ab.dst\", \"ab.relationship\",\"b\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can not infer schema for type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-c6e6d0c74e4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchain4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"a.id\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"a.name\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"ab.src\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ab.dst\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ab.relationship\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"b\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"b\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"f\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"inner\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\pyspark\\sql\\context.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[0mPy4JJavaError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \"\"\"\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    746\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 748\u001b[1;33m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    749\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[1;34m(self, data, schema)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m             \u001b[0mstruct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[1;34m(self, data, names)\u001b[0m\n\u001b[0;32m    346\u001b[0m             warnings.warn(\"inferring schema from dict is deprecated,\"\n\u001b[0;32m    347\u001b[0m                           \"please use pyspark.sql.Row instead\")\n\u001b[1;32m--> 348\u001b[1;33m         \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    346\u001b[0m             warnings.warn(\"inferring schema from dict is deprecated,\"\n\u001b[0;32m    347\u001b[0m                           \"please use pyspark.sql.Row instead\")\n\u001b[1;32m--> 348\u001b[1;33m         \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\pyspark\\sql\\types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[1;34m(row, names)\u001b[0m\n\u001b[0;32m   1060\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1061\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1062\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1063\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1064\u001b[0m     \u001b[0mfields\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Can not infer schema for type: <class 'str'>"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|id |name |\n",
      "+---+-----+\n",
      "|a  |Alice|\n",
      "|f  |Funny|\n",
      "+---+-----+\n",
      "\n",
      "+---+-----+---+---+------------+----------------+---+-----+\n",
      "| id| name|src|dst|relationship|               b| id| name|\n",
      "+---+-----+---+---+------------+----------------+---+-----+\n",
      "|  f|Fanny|  f|  c|      follow|[c, Charlie, 30]|  f|Funny|\n",
      "|  a|Alice|  a|  e|      friend| [e, Esther, 32]|  a|Alice|\n",
      "|  a|Alice|  a|  b|      friend|    [b, Bob, 36]|  a|Alice|\n",
      "|  a|Alice|  a|  e|      friend| [e, Esther, 32]|  a|Alice|\n",
      "+---+-----+---+---+------------+----------------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "mylist = [\n",
    "  {\"id\":\"a\",\"name\":\"Alice\"},\n",
    "  {\"id\":\"f\",\"name\":\"Funny\"}\n",
    "]\n",
    "df2 = sqlContext.createDataFrame(Row(**x) for x in mylist)\n",
    "df2.show(truncate=False)\n",
    "\n",
    "df1 = chain4.select(\"a.id\",\"a.name\",\"ab.src\", \"ab.dst\", \"ab.relationship\",\"b\")\n",
    "\n",
    "df1.join(df2,[df1.id==df2.id], \"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zqy\\Anaconda3\\lib\\pyspark\\sql\\session.py:346: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|id |name |\n",
      "+---+-----+\n",
      "|a  |Alice|\n",
      "|f  |Funny|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.createDataFrame(mylist).show(truncate=False)\n",
    "\n",
    "'''\n",
    "字典直接createDataFrame会报错\n",
    "C:\\Users\\zqy\\Anaconda3\\lib\\pyspark\\sql\\session.py:346: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
    "  warnings.warn(\"inferring schema from dict is deprecated,\"\n",
    "  \n",
    "  (Row(**x) for x in mylist) 构造的是一个生成器对象<class 'generator'>, 不同于套[]创建list对象[Row(id='a', name='Alice'), Row(id='f', name='Funny')]，generator保存的只是算法从⽽节省⼤量的空间\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+----------------+--------------+----------------+--------------+----------------+---+\n",
      "|               a|            ab|               b|            bc|               c|            cd|               d| id|\n",
      "+----------------+--------------+----------------+--------------+----------------+--------------+----------------+---+\n",
      "|  [d, David, 29]|[d, a, friend]|  [a, Alice, 34]|[a, e, friend]| [e, Esther, 32]|[e, f, follow]|  [f, Fanny, 36]|  d|\n",
      "| [e, Esther, 32]|[e, d, friend]|  [d, David, 29]|[d, a, friend]|  [a, Alice, 34]|[a, e, friend]| [e, Esther, 32]|  e|\n",
      "|  [d, David, 29]|[d, a, friend]|  [a, Alice, 34]|[a, e, friend]| [e, Esther, 32]|[e, d, friend]|  [d, David, 29]|  d|\n",
      "|  [a, Alice, 34]|[a, e, friend]| [e, Esther, 32]|[e, f, follow]|  [f, Fanny, 36]|[f, c, follow]|[c, Charlie, 30]|  a|\n",
      "|  [f, Fanny, 36]|[f, c, follow]|[c, Charlie, 30]|[c, b, follow]|    [b, Bob, 36]|[b, c, follow]|[c, Charlie, 30]|  f|\n",
      "|    [b, Bob, 36]|[b, c, follow]|[c, Charlie, 30]|[c, b, follow]|    [b, Bob, 36]|[b, c, follow]|[c, Charlie, 30]|  b|\n",
      "|  [d, David, 29]|[d, a, friend]|  [a, Alice, 34]|[a, b, friend]|    [b, Bob, 36]|[b, c, follow]|[c, Charlie, 30]|  d|\n",
      "| [e, Esther, 32]|[e, f, follow]|  [f, Fanny, 36]|[f, c, follow]|[c, Charlie, 30]|[c, b, follow]|    [b, Bob, 36]|  e|\n",
      "|[c, Charlie, 30]|[c, b, follow]|    [b, Bob, 36]|[b, c, follow]|[c, Charlie, 30]|[c, b, follow]|    [b, Bob, 36]|  c|\n",
      "|  [a, Alice, 34]|[a, b, friend]|    [b, Bob, 36]|[b, c, follow]|[c, Charlie, 30]|[c, b, follow]|    [b, Bob, 36]|  a|\n",
      "| [e, Esther, 32]|[e, d, friend]|  [d, David, 29]|[d, a, friend]|  [a, Alice, 34]|[a, b, friend]|    [b, Bob, 36]|  e|\n",
      "|  [a, Alice, 34]|[a, e, friend]| [e, Esther, 32]|[e, d, friend]|  [d, David, 29]|[d, a, friend]|  [a, Alice, 34]|  a|\n",
      "+----------------+--------------+----------------+--------------+----------------+--------------+----------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chain4.select(\"*\",\"a.id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+\n",
      "| id|  name|age|\n",
      "+---+------+---+\n",
      "|  e|Esther| 32|\n",
      "|  b|   Bob| 36|\n",
      "|  a| Alice| 34|\n",
      "+---+------+---+\n",
      "\n",
      "+---+---+------------+\n",
      "|src|dst|relationship|\n",
      "+---+---+------------+\n",
      "|  a|  e|      friend|\n",
      "|  a|  b|      friend|\n",
      "+---+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Subgraphs 子图\n",
    "g2 = g.filterEdges(\"relationship = 'friend'\").filterVertices(\"age > 30\").dropIsolatedVertices()\n",
    "g2.vertices.show()\n",
    "g2.edges.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------+\n",
      "|           from|            e0|            to|\n",
      "+---------------+--------------+--------------+\n",
      "|[e, Esther, 32]|[e, d, friend]|[d, David, 29]|\n",
      "+---------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standard graph algorithms 标准图算法\n",
    "# Breadth-first search 广度优先搜索BFS \n",
    "paths = g.bfs(\"name = 'Esther'\", \"age < 32\")\n",
    "paths.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------+--------------+----------------+\n",
      "|           from|            e0|            v1|            e1|              to|\n",
      "+---------------+--------------+--------------+--------------+----------------+\n",
      "|[e, Esther, 32]|[e, f, follow]|[f, Fanny, 36]|[f, c, follow]|[c, Charlie, 30]|\n",
      "+---------------+--------------+--------------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filteredPaths = g.bfs(         \\\n",
    "  fromExpr = \"name = 'Esther'\", \n",
    "  toExpr = \"age < 32\",\n",
    "  edgeFilter = \"relationship != 'friend'\",\n",
    "  maxPathLength = 3)\n",
    "filteredPaths.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------+\n",
      "|           from|            e0|            to|\n",
      "+---------------+--------------+--------------+\n",
      "|[e, Esther, 32]|[e, f, follow]|[f, Fanny, 36]|\n",
      "|[e, Esther, 32]|[e, d, friend]|[d, David, 29]|\n",
      "+---------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filteredPaths_60 = g.bfs(         \\\n",
    "  fromExpr = \"name = 'Esther'\", \n",
    "  toExpr = \"age < 60 and name != 'Esther'\",\n",
    "  maxPathLength = 3)\n",
    "filteredPaths_60.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------------+\n",
      "|id |distances                                       |\n",
      "+---+------------------------------------------------+\n",
      "|a  |[e -> 1, f -> 2, a -> 0, b -> 1, c -> 2, d -> 2]|\n",
      "|b  |[b -> 0, c -> 1]                                |\n",
      "|c  |[c -> 0, b -> 1]                                |\n",
      "|d  |[e -> 2, f -> 3, a -> 1, b -> 2, c -> 3, d -> 0]|\n",
      "|e  |[e -> 0, f -> 1, a -> 2, b -> 3, c -> 2, d -> 1]|\n",
      "|f  |[f -> 0, c -> 1, b -> 2]                        |\n",
      "|g  |[]                                              |\n",
      "+---+------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_60 = g.shortestPaths([\"a\", \"b\", \"c\",\"d\", \"e\", \"f\"])\n",
    "result_60.sort([\"id\"]).select(\"id\", \"distances\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "| id|key|value|\n",
      "+---+---+-----+\n",
      "|  a|  e|    1|\n",
      "|  a|  f|    2|\n",
      "|  a|  a|    0|\n",
      "|  a|  b|    1|\n",
      "|  a|  c|    2|\n",
      "|  a|  d|    2|\n",
      "|  b|  b|    0|\n",
      "|  b|  c|    1|\n",
      "|  c|  c|    0|\n",
      "|  c|  b|    1|\n",
      "|  d|  e|    2|\n",
      "|  d|  f|    3|\n",
      "|  d|  a|    1|\n",
      "|  d|  b|    2|\n",
      "|  d|  c|    3|\n",
      "|  d|  d|    0|\n",
      "|  e|  e|    0|\n",
      "|  e|  f|    1|\n",
      "|  e|  a|    2|\n",
      "|  e|  b|    3|\n",
      "+---+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "result_60.sort([\"id\"]).select(\"id\", explode(\"distances\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "| id|key|value|\n",
      "+---+---+-----+\n",
      "|  a|  e|    1|\n",
      "|  a|  f|    2|\n",
      "|  a|  b|    1|\n",
      "|  a|  c|    2|\n",
      "|  a|  d|    2|\n",
      "|  b|  c|    1|\n",
      "|  c|  b|    1|\n",
      "+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "set_指定本方 = [\"a\", \"b\", \"c\"]\n",
    "df_path = result_60.sort([\"id\"]).select(\"id\", explode(\"distances\"))\n",
    "df_指定source = df_path.filter(col(\"id\").isin(set_指定本方 ))\n",
    "df_指定深度 = df_指定source.filter(\"value>0 and value<3\")\n",
    "df_指定深度.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, name: string, age: bigint, distances: map<string,int>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(result_60.select(\"distances\")))\n",
    "result_60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-52da9b39144b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresult_60\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"name\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexplode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"distances\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"name\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'length' is not defined"
     ]
    }
   ],
   "source": [
    "result_60.sort([\"id\"]).select(\"id\",\"name\", explode(\"distances\")).filter(length(\"name\")<5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: struct<id:string,name:string,age:bigint>, e: struct<src:string,dst:string,relationship:string>, b: struct<id:string,name:string,age:bigint>, e2: struct<src:string,dst:string,relationship:string>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "motifs = g.find(\"(a)-[e]->(b); (b)-[e2]->(a)\")\n",
    "display(motifs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "[Row(a=Row(id='d', name='David', age=29), ab=Row(src='d', dst='a', relationship='friend'), b=Row(id='a', name='Alice', age=34), bc=Row(src='a', dst='e', relationship='friend'), c=Row(id='e', name='Esther', age=32), cd=Row(src='e', dst='f', relationship='follow'), d=Row(id='f', name='Fanny', age=36)), Row(a=Row(id='e', name='Esther', age=32), ab=Row(src='e', dst='d', relationship='friend'), b=Row(id='d', name='David', age=29), bc=Row(src='d', dst='a', relationship='friend'), c=Row(id='a', name='Alice', age=34), cd=Row(src='a', dst='e', relationship='friend'), d=Row(id='e', name='Esther', age=32)), Row(a=Row(id='d', name='David', age=29), ab=Row(src='d', dst='a', relationship='friend'), b=Row(id='a', name='Alice', age=34), bc=Row(src='a', dst='e', relationship='friend'), c=Row(id='e', name='Esther', age=32), cd=Row(src='e', dst='d', relationship='friend'), d=Row(id='d', name='David', age=29))]\n"
     ]
    }
   ],
   "source": [
    "chain = g.find(\"(a)-[ab]->(b);(b)-[bc]->(c);(c)-[cd]->(d)\") \n",
    "print(type(chain))\n",
    "print(chain.head(3))\n",
    "'''\n",
    "# 定义更新状态条件，关系为friends则+1\n",
    "sumChain = lambda cnt, relation: when(\\\n",
    "    relationship == 'friends', cnt + 1) \\\n",
    "  .otherwise(cnt)\n",
    "# 应用到chain，计算好友数量\n",
    "condition = reduce(lambda cnt,sumChain(cnt, col(e).relationship),[\"ab\", \"bc\", \"cd\"],lit(0))\n",
    "\n",
    "# 计算好友圈数\n",
    "chainWithFriends = chain.where(condition >= 2)\n",
    "chainWithFriends.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+----------------+--------------+\n",
      "|               a|             e|               b|            e2|\n",
      "+----------------+--------------+----------------+--------------+\n",
      "|[c, Charlie, 30]|[c, b, follow]|    [b, Bob, 36]|[b, c, follow]|\n",
      "|    [b, Bob, 36]|[b, c, follow]|[c, Charlie, 30]|[c, b, follow]|\n",
      "+----------------+--------------+----------------+--------------+\n",
      "\n",
      "+----------------+--------------+----------------+--------------+\n",
      "|               a|             e|               b|            e2|\n",
      "+----------------+--------------+----------------+--------------+\n",
      "|[c, Charlie, 30]|[c, b, follow]|    [b, Bob, 36]|[b, c, follow]|\n",
      "|    [b, Bob, 36]|[b, c, follow]|[c, Charlie, 30]|[c, b, follow]|\n",
      "+----------------+--------------+----------------+--------------+\n",
      "\n",
      "+----------------+\n",
      "|           start|\n",
      "+----------------+\n",
      "|  [f, Fanny, 36]|\n",
      "| [e, Esther, 32]|\n",
      "| [e, Esther, 32]|\n",
      "|  [d, David, 29]|\n",
      "|[c, Charlie, 30]|\n",
      "|    [b, Bob, 36]|\n",
      "|  [a, Alice, 34]|\n",
      "|  [a, Alice, 34]|\n",
      "+----------------+\n",
      "\n",
      "+---------------+----------------+\n",
      "|              a|               b|\n",
      "+---------------+----------------+\n",
      "| [a, Alice, 34]| [e, Esther, 32]|\n",
      "|[e, Esther, 32]|  [d, David, 29]|\n",
      "|[e, Esther, 32]|  [f, Fanny, 36]|\n",
      "| [a, Alice, 34]|    [b, Bob, 36]|\n",
      "| [f, Fanny, 36]|[c, Charlie, 30]|\n",
      "| [d, David, 29]|  [a, Alice, 34]|\n",
      "+---------------+----------------+\n",
      "\n",
      "+---+---+\n",
      "|  A|  C|\n",
      "+---+---+\n",
      "|  e|  c|\n",
      "|  e|  a|\n",
      "|  d|  b|\n",
      "|  a|  d|\n",
      "|  f|  b|\n",
      "|  d|  e|\n",
      "|  a|  f|\n",
      "|  a|  c|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# import pandas as pd\n",
    "# from pyspark.sql import SparkSession\n",
    "#\n",
    "# spark = SparkSession \\\n",
    "#     .builder \\\n",
    "#     .appName('pyspark') \\\n",
    "#     .getOrCreate()\n",
    "# # 原始数据\n",
    "# test = spark.createDataFrame([('001','1',100,87,67,83,98), ('002','2',87,81,90,83,83), ('003','3',86,91,83,89,63),\n",
    "#                             ('004','2',65,87,94,73,88), ('005','1',76,62,89,81,98), ('006','3',84,82,85,73,99),\n",
    "#                             ('007','3',56,76,63,72,87), ('008','1',55,62,46,78,71), ('009','2',63,72,87,98,64)],\n",
    "# \t\t\t\t\t\t\t['number','class','language','math','english','physic','chemical'])\n",
    "# test.show()\n",
    "\n",
    "# import pyspark\n",
    "# from pyspark import SparkContext, SparkConf\n",
    "# conf = SparkConf().setAppName(\"test\").setMaster(\"local[4]\")\n",
    "# sc = SparkContext(conf=conf)\n",
    "#\n",
    "# #只需要5行代码就可以完成WordCount词频统计。\n",
    "# file_path = 'D:/spark/eat_pyspark_in_10_days/data/hello.txt'\n",
    "# rdd_line = sc.textFile(file_path)\n",
    "# rdd_word = rdd_line.flatMap(lambda x:x.split(\" \"))\n",
    "# rdd_one = rdd_word.map(lambda t:(t,1))\n",
    "# rdd_count = rdd_one.reduceByKey(lambda x,y:x+y)\n",
    "# print(rdd_count.collect())\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('pyspark') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "vertices = spark.createDataFrame([\n",
    "    (\"a\", \"Alice\", 34),\n",
    "    (\"b\", \"Bob\", 36),\n",
    "    (\"c\", \"Charlie\", 30),\n",
    "    (\"d\", \"David\", 29),\n",
    "    (\"e\", \"Esther\", 32),\n",
    "    (\"f\", \"Fanny\", 36),\n",
    "    (\"g\", \"Gabby\", 60)], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "edges = spark.createDataFrame([\n",
    "    (\"a\", \"b\", \"friend\"),\n",
    "    (\"b\", \"c\", \"follow\"),\n",
    "    (\"c\", \"b\", \"follow\"),\n",
    "    (\"f\", \"c\", \"follow\"),\n",
    "    (\"e\", \"f\", \"follow\"),\n",
    "    (\"e\", \"d\", \"friend\"),\n",
    "    (\"d\", \"a\", \"friend\"),\n",
    "    (\"a\", \"e\", \"friend\")\n",
    "], [\"src\", \"dst\", \"relationship\"])\n",
    "\n",
    "# 生成图\n",
    "graph = GraphFrame(vertices, edges)\n",
    "\n",
    "# print(\"顶点表视图：\")\n",
    "# graph.vertices.show() # graph.vertices 就是原始的vertices\n",
    "# 顶点表视图：\n",
    "# +---+-------+---+\n",
    "# | id|   name|age|\n",
    "# +---+-------+---+\n",
    "# |  a|  Alice| 34|\n",
    "# |  b|    Bob| 36|\n",
    "# |  c|Charlie| 30|\n",
    "# |  d|  David| 29|\n",
    "# |  e| Esther| 32|\n",
    "# |  f|  Fanny| 36|\n",
    "# |  g|  Gabby| 60|\n",
    "# +---+-------+---+\n",
    "\n",
    "# print(\"边表视图：\")\n",
    "# graph.edges.show() # graph.edges 就是原始的 edges\n",
    "# 边表视图：\n",
    "# +---+---+------------+\n",
    "# |src|dst|relationship|\n",
    "# +---+---+------------+\n",
    "# |  a|  b|      friend|\n",
    "# |  b|  c|      follow|\n",
    "# |  c|  b|      follow|\n",
    "# |  f|  c|      follow|\n",
    "# |  e|  f|      follow|\n",
    "# |  e|  d|      friend|\n",
    "# |  d|  a|      friend|\n",
    "# |  a|  e|      friend|\n",
    "# +---+---+------------+\n",
    "\n",
    "# print(\"三元组视图：\")\n",
    "# graph.triplets.show()\n",
    "# 三元组视图：\n",
    "# +----------------+--------------+----------------+\n",
    "# |             src|          edge|             dst|\n",
    "# +----------------+--------------+----------------+\n",
    "# | [e, Esther, 32]|[e, f, follow]|  [f, Fanny, 36]|\n",
    "# |  [a, Alice, 34]|[a, e, friend]| [e, Esther, 32]|\n",
    "# | [e, Esther, 32]|[e, d, friend]|  [d, David, 29]|\n",
    "# |  [f, Fanny, 36]|[f, c, follow]|[c, Charlie, 30]|\n",
    "# |    [b, Bob, 36]|[b, c, follow]|[c, Charlie, 30]|\n",
    "# |[c, Charlie, 30]|[c, b, follow]|    [b, Bob, 36]|\n",
    "# |  [a, Alice, 34]|[a, b, friend]|    [b, Bob, 36]|\n",
    "# |  [d, David, 29]|[d, a, friend]|  [a, Alice, 34]|\n",
    "# +----------------+--------------+----------------+\n",
    "\n",
    "# 顶点的度\n",
    "# graph.degrees.show()\n",
    "# +---+------+\n",
    "# | id|degree|\n",
    "# +---+------+\n",
    "# |  f|     2|\n",
    "# |  e|     3|\n",
    "# |  d|     2|\n",
    "# |  c|     3|\n",
    "# |  b|     3|\n",
    "# |  a|     3|\n",
    "# +---+------+\n",
    "\n",
    "# 顶点的入度\n",
    "# graph.inDegrees.show()\n",
    "# +---+--------+\n",
    "# | id|inDegree|\n",
    "# +---+--------+\n",
    "# |  f|       1|\n",
    "# |  e|       1|\n",
    "# |  d|       1|\n",
    "# |  c|       2|\n",
    "# |  b|       2|\n",
    "# |  a|       1|\n",
    "# +---+--------+\n",
    "\n",
    "# 顶点的出度\n",
    "# graph.outDegrees.show()\n",
    "# +---+---------+\n",
    "# | id|outDegree|\n",
    "# +---+---------+\n",
    "# |  f|        1|\n",
    "# |  e|        2|\n",
    "# |  d|        1|\n",
    "# |  c|        1|\n",
    "# |  b|        1|\n",
    "# |  a|        2|\n",
    "# +---+---------+\n",
    "\n",
    "# 多个路径条件\n",
    "motif = graph.find(\"(a)-[e]->(b); (b)-[e2]->(a)\")\n",
    "motif.show()\n",
    "\n",
    "# 在搜索的结果上进行过滤\n",
    "motif.filter(\"b.age > 30\")\n",
    "motif.show()\n",
    "\n",
    "# 不需要返回路径中的元素时，可以使用匿名顶点和边\n",
    "motif = graph.find(\"(start)-[]->()\")\n",
    "motif.show()\n",
    "\n",
    "# 设置路径不存在的条件\n",
    "motif = graph.find(\"(a)-[]->(b); !(b)-[]->(a)\")\n",
    "motif.show()\n",
    "\n",
    "# Motif: A->B->C but not A->C\n",
    "results = graph.find(\"(A)-[]->(B); (B)-[]->(C); !(A)-[]->(C)\")\n",
    "# 排除自己\n",
    "results = results.filter(\"A.id != C.id\")\n",
    "# 选择需要的列\n",
    "results = results.select(results.A.id.alias(\"A\"), results.C.id.alias(\"C\"))\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark version: 2.4.6\n",
      "hello spark\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(\"spark version:\",pyspark.__version__)\n",
    "rdd = sc.parallelize([\"hello\",\"spark\"])\n",
    "print(rdd.reduce(lambda x,y:x+' '+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
